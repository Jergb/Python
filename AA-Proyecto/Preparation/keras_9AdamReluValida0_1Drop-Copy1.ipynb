{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-bf26b2c1a740>, line 271)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-4-bf26b2c1a740>\"\u001b[1;36m, line \u001b[1;32m271\u001b[0m\n\u001b[1;33m    CAmbiamos todos los mse por mae :D\u001b[0m\n\u001b[1;37m                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "## Vamo a importar los datos\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "temp = pd.read_csv('nodo2_periodo3.csv')#,parse_dates=True,index_col=0)\n",
    "import math\n",
    "\n",
    "temp.head()\n",
    "\n",
    "y = temp.iloc[:,4]\n",
    "display(y.head())\n",
    "display(y.tail())\n",
    "\n",
    "from statsmodels.tsa.stattools import pacf\n",
    "import matplotlib.pyplot as plt\n",
    "x_pacf = pacf(y, nlags=30, method='ols')\n",
    "plt.plot(x_pacf)\n",
    "plt.plot(len(x_pacf)*[0],'--')\n",
    "plt.title(' Partial autocorrelation function for Monthly Sunspots')\n",
    "plt.show()\n",
    "\n",
    "ventana = 10\n",
    "x = pd.DataFrame(index=range(len(y)))\n",
    "for i in range(ventana):\n",
    "    x[i] = y.shift(i+1)\n",
    "display(x.head(6))\n",
    "x = x.dropna()\n",
    "display(x.head(6))\n",
    "display(x.tail(6))\n",
    "\n",
    "y = y[ventana:]\n",
    "display(y.head())\n",
    "display(y.tail())\n",
    "display(x.head())\n",
    "display(x.tail())\n",
    "\n",
    "## Vamo a normalizar los datos\n",
    "\n",
    "from sklearn import preprocessing\n",
    "# scaler para x\n",
    "scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# 5 columnas\n",
    "x = np.array(x).reshape((len(x),ventana))\n",
    "x = scaler.fit_transform(x)\n",
    "\n",
    "# scaler para y\n",
    "scaler_y = preprocessing.MinMaxScaler(feature_range=(0,1))\n",
    "y = np.array(y).reshape((len(y),1))\n",
    "y = scaler_y.fit_transform(y)\n",
    "\n",
    "# Train and Test Sets\n",
    "\n",
    "#Define la forma de entrada y salida de los datos, las variables usadas para predecir y la variable que se va a predecir\n",
    "\n",
    "train_end = int(len(y)*.8)\n",
    "x_train = x[0:train_end,]\n",
    "display('x_train: ',x_train[0:5])\n",
    "x_test = x[train_end+1:len(y),]\n",
    "y_train = y[0:train_end]\n",
    "y_test = y[train_end+1:len(y)]\n",
    "display('y_train: ',y_train[0:5])\n",
    "x_train = x_train.reshape(x_train.shape+(1,))\n",
    "x_test = x_test.reshape(x_test.shape+(1,))\n",
    "# Set de validación\n",
    "y_valid = y_train [train_end-50:train_end]\n",
    "x_valid = x_train [train_end-50:train_end]\n",
    "\n",
    "print('Shape of x_train is ', x_train.shape)\n",
    "print('Shape of x_test is ', x_test.shape)\n",
    "\n",
    "# importa el modelo secuencial, permite apilar capas de manera lineal\n",
    "from keras.models import Sequential\n",
    "# importa la capa densa, esta es una capa de red neuronal completamente conectada regular\n",
    "# con una función lineal de activación\n",
    "from keras.layers import Dense, Activation\n",
    "# Optimizador de descenso de gradiente estocástico\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras . layers import Dropout\n",
    "\n",
    "## Vamo a definir el modelo\n",
    "\n",
    "# define una semilla para tener resultados repetibles\n",
    "seed = 1161180\n",
    "np.random.seed(seed)\n",
    "# Define el modelo secuencial\n",
    "fit1 = Sequential()\n",
    "# Agrega una capa LSTM con cuatro neuronas, activación tanh, activación interoir hard_sgimoitde\n",
    "# y forma de entrada (5,1)\n",
    "fit1.add(LSTM(units=4,activation='relu',\n",
    "              recurrent_activation='hard_sigmoid',\n",
    "              input_shape=(ventana,1)))\n",
    "# añade 5% drop out\n",
    "fit1.add(Dropout(0.1))\n",
    "#Agrega una capa densa con una slida y activación lineal\n",
    "fit1.add(Dense(units=1, activation='linear'))\n",
    "\n",
    "# Define el modelo secuencial\n",
    "fit2 = Sequential()\n",
    "# Agrega una capa LSTM con cuatro neuronas, activación tanh, activación interoir hard_sgimoitde\n",
    "# y forma de entrada (5,1)\n",
    "fit2.add(LSTM(units=4,activation='relu',\n",
    "              recurrent_activation='hard_sigmoid',\n",
    "              input_shape=(ventana,1)))\n",
    "#Agrega 5% de drop out\n",
    "fit2.add(Dropout(0.1))\n",
    "#Agrega una capa densa con una slida y activación lineal\n",
    "fit2.add(Dense(units=1, activation='linear'))\n",
    "\n",
    "fit1.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "#Ajustar el modelo\n",
    "history1 = fit1.fit(x_train, y_train, batch_size = 1,epochs=10,\n",
    "                    validation_data=(x_valid,y_valid),\n",
    "                    callbacks =[EarlyStopping(monitor ='val_loss',patience=100,\n",
    "                                              verbose=0,mode='auto')],verbose=0)\n",
    "\n",
    "fit2.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "#Ajustar el modelo\n",
    "history2 = fit2.fit(x_train, y_train, batch_size = 1, epochs=10, shuffle=True,\n",
    "                    validation_data=(x_valid,y_valid),\n",
    "                    callbacks =[EarlyStopping(monitor ='val_loss',patience=100,\n",
    "                                              verbose=0,mode='auto')],verbose=0)\n",
    "\n",
    "fit3 = Sequential ()\n",
    "# The batch_input_shape takes the batch size (1 in our example), number of attributes \n",
    "#(5 time lagged variables) and number of time steps (1 month forecast).\n",
    "fit3.add(LSTM(units=4,stateful=True,batch_input_shape=(1,ventana,1),activation='relu',\n",
    "              recurrent_activation ='hard_sigmoid'))\n",
    "fit3.add(Dropout(0.1))\n",
    "fit3.add(Dense(units=1,activation='linear'))\n",
    "fit3.compile(loss='mean_absolute_error',optimizer='adam')\n",
    "\n",
    "fit4 = Sequential ()\n",
    "# The batch_input_shape takes the batch size (1 in our example), number of attributes \n",
    "#(5 time lagged variables) and number of time steps (1 month forecast).\n",
    "fit4.add(LSTM(units=4,stateful=True,batch_input_shape=(1,ventana,1),activation='relu',\n",
    "              recurrent_activation ='hard_sigmoid'))\n",
    "#fit4.add(Dropout(0.1))\n",
    "fit4.add(Dense(units=1,activation='linear'))\n",
    "fit4.compile(loss='mean_absolute_error',optimizer='adam')\n",
    "\n",
    "end_point =len(x_train)\n",
    "start_point =end_point - 50\n",
    "\n",
    "#The model has to be trained one epoch at a time with the\n",
    "#state reset after each epoch\n",
    "\n",
    "history3=[]\n",
    "for i in range(len(x_train[start_point:end_point])):\n",
    "    history3.append(fit3.fit(x_train[start_point:end_point],y_train[start_point:end_point],epochs=1,\n",
    "             batch_size=1,shuffle=False,validation_data=(x_valid,y_valid),\n",
    "                             callbacks =[EarlyStopping(monitor ='val_loss',patience=100,\n",
    "                                                       verbose=0,mode='auto')],verbose=0))\n",
    "    fit3.reset_states()\n",
    "    \n",
    "history4=[]\n",
    "for i in range(len(x_train[start_point:end_point])):\n",
    "    history4.append(fit4.fit(x_train[start_point:end_point],y_train[start_point:end_point],epochs=1,\n",
    "             batch_size=1,shuffle=True, validation_data=(x_valid,y_valid),\n",
    "                             callbacks =[EarlyStopping(monitor ='val_loss',patience=100,\n",
    "                                                       verbose=0,mode='auto')],verbose=0))\n",
    "    fit4.reset_states()\n",
    "\n",
    "from keras.layers.recurrent import GRU\n",
    "\n",
    "fit5 = Sequential ()\n",
    "#return_sequeinces=False para trabajar con una sola objetivo\n",
    "#Para trabajar con varios objetivos se deja en True\n",
    "fit5.add(GRU(units=4,return_sequences= False,activation='relu',\n",
    "             recurrent_activation='hard_sigmoid',input_shape=(ventana,1)))\n",
    "fit1.add(Dropout(0.05))\n",
    "fit5.add(Dense(units=1,activation='linear'))\n",
    "fit5.compile(loss='mean_absolute_error',optimizer='adam')\n",
    "\n",
    "history5 = fit5.fit(x_train,y_train,batch_size=1,epochs=10, validation_data=(x_valid,y_valid),\n",
    "                    callbacks =[EarlyStopping(monitor ='val_loss',patience=100,\n",
    "                                              verbose=0,mode='auto')],verbose=0)\n",
    "\n",
    "#the\n",
    "#model will forecast the next month based on the last 500 rolling\n",
    "#months of data:\n",
    "\n",
    "#The larger the batch size, the more memory you\n",
    "#will need to run the model\n",
    "\n",
    "display(fit1.summary())\n",
    "display(fit2.summary())\n",
    "display(fit3.summary())\n",
    "display(fit4.summary())\n",
    "display(fit5.summary())\n",
    "\n",
    "# Evaluar el modelo\n",
    "#Train and Test MSE\n",
    "\n",
    "\n",
    "for i in range(1,6):\n",
    "    print('fit%d'%i)\n",
    "    f = vars()[('fit%s')%str(i)]\n",
    "    tr = vars()[('score_train%s')%str(i)] = f.evaluate(x_train, y_train, batch_size=1)\n",
    "    ts = vars()[('score_test%s')%str(i)] = f.evaluate(x_test, y_test, batch_size=1)\n",
    "    print('in train%d MAE = '%i, round(tr,4))\n",
    "    print('in test%d MAE = '%i, round(ts,4),'\\n')\n",
    "\n",
    "#And to convert the predictions back to their original scale,\n",
    "#so we can view them individually:\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "entreno = scaler_y.inverse_transform(np.array(y_test).reshape((len(y_test),1)))\n",
    "plt.figure(figsize=[16,8])\n",
    "plt.subplot(3,2,6)\n",
    "plt.plot(np.rint(entreno),label='set de prueba',linewidth=2)\n",
    "plt.title('Set de prueba')\n",
    "\n",
    "for i in [1,2,5]:\n",
    "    plt.subplot(3,2,i)\n",
    "    f = vars()[('fit%s')%str(i)]\n",
    "    p = f.predict(x_test)\n",
    "    p = vars()[('pred%s')%str(i)] = scaler_y.inverse_transform(np.array(p).reshape((len(p),1)))\n",
    "    p1 = mean_absolute_error(p, entreno)\n",
    "    plt.plot(np.rint(p))\n",
    "    plt.plot(abs(p-entreno),alpha=.3)\n",
    "    plt.title('fit%d\\n MAE = %.2f'%(i,p1))\n",
    "\n",
    "for i in [3,4]:\n",
    "    plt.subplot(3,2,i)\n",
    "    f = vars()[('fit%s')%str(i)]\n",
    "    p = f.predict(x_test,batch_size=1)\n",
    "    p = vars()[('pred%s')%str(i)] = scaler_y.inverse_transform(np.array(p).reshape((len(p),1)))\n",
    "    p1 = mean_absolute_error(p, entreno)\n",
    "    plt.plot(np.rint(p))\n",
    "    plt.title('fit%d\\n MAE = %.2f'%(i,p1))\n",
    "    plt.plot(abs(p-entreno),alpha=.3)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.figure(figsize=[16,8])\n",
    "for i in range(1,6):\n",
    "    plt.subplot(3,2,i)\n",
    "    p = vars()[('pred%s')%str(i)]\n",
    "    plt.scatter(np.rint(p),np.rint(entreno),alpha=.1)\n",
    "    plt.title('fit%d'%i)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.figure(figsize=[16,4])\n",
    "plt.subplot(1,3,1)\n",
    "plt.plot(history1.history['loss'], 'r.-', label='pérdidas sin shuffle')\n",
    "plt.plot(history2.history['loss'], 'b.-', label='pérdidas con shuffle')\n",
    "plt.plot(history5.history['loss'], 'g.-', label='pérdidas con GRU')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "h3 = [x.history['loss'] for x in history3]\n",
    "plt.plot(h3, 'r.-', label='pérdidas con estado sin shuffle')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "h4 = [x.history['loss'] for x in history4]\n",
    "plt.plot(h4, 'b.-',label='pérdidas con estado con shuffle')\n",
    "plt.legend()\n",
    "\n",
    "fit1.save('lstm.h5')\n",
    "fit2.save('lstms.h5')\n",
    "fit3.save('lstm_s.h5')\n",
    "fit4.save('lstm_ss.h5')\n",
    "fit5.save('grus.h5')\n",
    "from sklearn.externals import joblib \n",
    "joblib.dump(scaler_y, 'scaler.save')\n",
    "\n",
    "#CAmbiamos todos los mse por mae :D\n",
    "#al cuarto le quitamos la capa Dropout"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
